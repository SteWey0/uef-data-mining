\subsection*{Lecture}
The lectures this week dealt with introductory topics regarding the structure of this course, a math and statistics rehearsal as well as an introduction to the topic Data Mining. 
The math and statistics chapter covers many basic definitions like the axioms and basics properties of probabilities, the definitions of partial derivatives, vectors and matrices and their addition and multiplication properties. 
Although these are nice to have for completeness sake and should prove helpful for students which do not have a background in statistics yet, for me, they were already known and will therefore not be repeated here. 
Instead, I will focus on definitions of this chapter which I do not know by heart and which I think will prove useful for the following and will put them into context of what I have already learned. \\

The sample mean $\hat{\mu}$ and sample variance $\hat{\sigma}^2$ provide unbiased estimators for the population mean $\mu$ and population variance $\sigma^2$ given a certain sample $v_i$ of size $N$, i.e. $i\in(1,N)$. 
They are defined as 
\begin{gather}
    \hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} v_I  
    \label{eq:sample mean}\\
    \hat{\sigma}^2 = \frac{1}{N-1} \sum_{i=1}^{N} \left( v_i - \hat{\mu}\right)^2
    \label{eq:sample variance}
\end{gather}
I recall from a prior course on statistics that the sample variance with the $N-1$ term in the denominator should be used in the case of an \emph{unknown} mean, i.e. if the mean is estimated by \autoref{eq:sample mean}, as it provides an unbiased estimator and therefore better convergence to the true but unknown sample variance for small $N$. 
If instead the mean is inferred through different means, the minus 1 term can be dropped. 
In \lstinline{numpy} the sample variance can be simply calculated by setting the parameter \lstinline{ddof=1}:

\begin{lstlisting}[language=Python, caption={Sample variance, calculated in numpy}, label={lst:sample variance}]
import numpy as np
# array is a sample array of data
sample_var = np.std(array, ddof=1)
\end{lstlisting}

If there are more than just one random variates a variance can be calculated for each one. 
Additionally, a so-called covariance between two different variated can also be calculated. 
Covariances and variances are summarized in a covariance matrix, whose elements are defined as follows: 

\begin{equation}
    \mathrm{Cov}\left(x,y\right) = \frac{1}{n-1} \sum_{i=1}^{N}\left(x_i-\hat{\mu}_x\right)\left(y_i-\hat{\mu}_y\right) = \hat{\sigma}_x\hat{\sigma}_y\rho_{x,y}
    \label{eq:covariance matrix}
\end{equation}
Here $\hat{\sigma}_i$ are the standard deviations of the variates $x$ and $y$ according to \autoref{eq:sample variance} and $\rho_{x,y}$ is the degree of correlation between $x$ and $y$. 
It holds that $\rho_{x,y}$ is always between $-1$ and 1 with $-1$ indicating maximum negative correlation, 1 maximum positive correlation and 0 no correlation at all. 
The covariance matrix (or the reduced correlation matrix obtained when removing all individual standard deviations) therefore indicates correlations between different parameters in a dataset which can be used for explorative data analysis. 
Another use of it is when fitting a model to a dataset. 
Here a large degree of correlation between model parameters indicates a surplus of model parameters. \\

The normal distribution is the most important continuos probability distribution as a lot of measured data follows it. 
This is due to the central limit theorem stating that means of random variables taken from arbitrary probability distributions will be distributed normally. 
As measured quantities are usually means over some finite measurement integration time due to a finite detector resolution many data are distributed normally. 
The normal or Gaussian probability distribution is given by
\begin{equation}
    P(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{\left(x-\mu\right)^2}{2\sigma^2}\right)
    \label{eq:normal distribution}
\end{equation}
with $\mu$ its mean and $\sigma$ its width, corresponding to the population standard deviation in $x$. 
It is a symmetric distribution and used to formulate confidence intervals as follows:
\begin{align}
    P\left(\mu-1\sigma<x<\mu+1\sigma\right) &\approx 68\% \nonumber\\
    P\left(\mu-2\sigma<x<\mu+2\sigma\right) &\approx 95.5\% \nonumber\\
    P\left(\mu-3\sigma<x<\mu+3\sigma\right) &\approx 99.7\% 
    \label{eq:confidence intervals normal distribution}
\end{align}
Further important definitions are those of the Jacobian and Hessian matrices, which are matrices of first order derivates of vector valued functions $\mathbf{f}(\mathbf{x})$ or second order derivates of scalar valued functions $f(\mathbf{x})$ respectively. 
They are used in optimization algorithms like data fitting or neural network learning. 
These methods are usually implemented already in various Python packages, however I still list the form of the Jacobian and Hessian here for completeness:
\begin{align}
    \mathbf{J} &= \nabla_x \cdot \mathbf{f}(\mathbf{x}) & &\text{or element-wise:} & J_{ij} &= \frac{\partial f_i}{\partial x_j} 
    \label{eq:jacobian} \\
    \mathbf{H} &= \nabla_x^2 f(\mathbf{x}) & &\text{or element-wise:} & H_{ij} &= \frac{\partial^2 f}{\partial x_i \partial x_j}
    \label{eq:hessian}
\end{align}
The last part of the lecture considered Data Mining. 
We discussed that Data Mining is a very structured process, where certain steps are done in order to hopefully get meaningful insights from the data at hand and be able to form a model that generalizes well. 
The first step is explorative data analysis (EDA), where plotting the data (e.g. in histograms or box plots) or calculating covariance matrices to get a rough idea about correlation are utilized. 
This step makes a lot of sense to me, especially once one deals with larger and larger datasets. 
However, this has been quite underutilized during my physics studies so far, as we have usually worked the other way around by first forming a hypothesis (for example a model) and then applying it to the data and doing statistics to see if it fits. 
It is quite interesting to me to do a different approach in this lecture and to see, where this may lead me. 
I am looking forward to applying this to real datasets in the following weeks during the tutorials.  
The next step is descriptive modelling, where one tries to find patters in the data. 
One can for example try to fit functions like multivariate Gaussians or use clustering methods on the data. 
Additionally, one can also try to use predictive modelling like classification neural networks to group the data and gather new insights. 
Hopefully these steps then lead to discovering patterns and rules in the data and finding a strong, i.e. simple model with high degree of generalization and prediction-power instead of remaining with weak models, which do offer some insights but fail to generalize and really understand the data. 
Weak models are also often classified by many parameters leading to the aforementioned overfitting. 
During my prior classes where I often used model fitting and machine learning I have often had contacts with overfitting due to fit functions with too many free parameters or overly large neural networks. 
I therefore know, that avoiding overfitting is one of the most difficult parts in data analysis and am definitely looking forward to learning more about this. 

\subsection*{Exercise}
The goal of the first exercise was to get a handle on the provided \lstinline|toolo.csv| dataset by using the aforementioned technique of EDA. 
First data from leap years is dropped for better comparability. 
After this the \lstinline|.describe| method of \lstinline|pandas|-dataframes is used to get a summary of the nitrogen dioxide and ozone concentration columns. 
By this we find mean concentrations and standard deviations of:
\begin{align*}
    \mathrm{NO_2} &: 38.4 \pm 23.2\,\mathrm{\frac{\mu g}{m^3}} \\
    \mathrm{O_3} &: 37.0 \pm 22.0\,\mathrm{\frac{\mu g}{m^3}} 
\end{align*}
This makes clear, that the nitrogen dioxide measurement has a larger variability than the ozone measurement. 
Furthermore, this simple analysis indicates already some erroneous data points. 
The minimum of the NO$_2$ concentration is for example at $-3\,\mathrm{\frac{\mu g}{m^3}}$. 
This is unphysical and could stem from an erroneous measurement e.g. an error when digitizing the measured concentration. 
Sometimes measurement devices also deliberately save unphysical values to indicate an error that happened during the measurement. 
In this case the value of $-3\,\mathrm{\frac{\mu g}{m^3}}$ could be understood as an error code. 
To see whether this is the case, one would need to take a look at the manufacturers data sheet of the specific detector in question. 
In principle such data points should be excluded from further analysis to not skew the results one gets from the physical data. \\
In a next step the columns corresponding to the particulate matter mass under 10\,$\mu$m, carbon monoxide concentration, temperature, humidity and wind speed have been analyzed additionally in a similar manner. 
From this it becomes clear, that the mass and carbon monoxide concentration show similar unphysical negative values as the nitrogen dioxide concentration. 
As also these unphysical values occur at perfect integer values, a look into the datasheet or the digitization software of the detectors would be helpful to understand these values and see, if they maybe really correspond to error codes. 
O course, filtering this data out is simple, but it is worth to investigate where this erroneous data stems from, what could be wrong during measurement and also how much data is affected. 
Additionally, there are some interesting insights to be gained by looking at the mean, standard deviation, minimum and maximum values of the data besides faulty measurements. 
One can for example see, that there is a seemingly small number of very heavy small particles in the analyzed air, as the mean value of the mass is roughly 25 times smaller than the maximum value. 
Furthermore, one can see that the carbon monoxide concentration in the air is much smaller than that of any other gas. 
The temperature and humidity show very large spreads over the dataset that are on the order of a forth of the whole data range. 
Lastly, one can see that the wind speed is a strictly positive quantity and is therefore not directional. 
This shows that already a simple analysis requiring only a few lines of code can give valuable insights on the data range, its variability and possible errors during measurement. \\

Next the correlation between the individual variates is calculated (compare \autoref{eq:covariance matrix}) and plotted. 
This is shown in \autoref{fig:plots/e1_corr.pdf}. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e1_corr.pdf}
    \caption{Cross correlation matrix calculated using the mentioned columns of the \lstinline|toodo.csv| dataset.}
    \label{fig:plots/e1_corr.pdf}
\end{figure}
The strongest positive correlation is present between the nitrogen dioxide and carbon monoxide concentration. 
This indicates, that the processing leading to an increase or decrease of these gases in the atmosphere are strongly linked such that an increase in one is connected to an increase in the other gas. 
At this point it is important to keep in mind that this analysis shows only correlation but not causality meaning that we cannot conclusively state which change leads to which but only that they are connected. 
Similarly, an anticorrelation between the concentrations of nitrogen dioxide and ozone can be seen as the correlation coefficient between those two quantities is negative. 
This indicates that a rise in one of the quantities is linked to a fall in the other. 
Additionally, a correlation between particle mass under ten microns and nitrogen dioxide concentration is present, suggesting a link between these two. 
There are also smaller degrees of anticorrelation between both the wind speed and nitrogen dioxide concentration as well as temperature and humidity. 
While the latter seems reasonable (higher temperatures seem to be connected with lower relative humidities), the prior is quite hard for me to understand. 
It could be interesting to investigate this further. 
It is quite fascinating to me, that such a simple analysis, requiring only a few simple lines of code can already extract interesting research questions that can be expanded on. \\

In a last task the individual distributions of the quantities was looked at by plotting histograms of the data shown in \autoref{fig:plots/e1_hist.pdf}. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e1_hist.pdf}
    \caption{Histogram of the data columns of the \lstinline|toodo.csv| dataset.}
    \label{fig:plots/e1_hist.pdf}
\end{figure}
While temperature as well as the gas concentrations and wind speed qualitatively look roughly normal distributed (taking into account, that the distribution is cut due to unphysical values), the humidity and particle masses under ten microns seem quite asymmetric. 
The histogram of the particle masses roughly fits to the realization I already had when looking at the mean, minimum and maximum values of the taken data in a prior step. 
It seems there are a lot of particles with low mass which influence the mean heavily, but there also remain a few particles of very high mass. 
In fact, to me the distribution looks similar to an exponential when postulating that very small masses are underrepresented probably due to the detector not being able to measure arbitrarily small particulates accurately. 
Of course, this would still need to be quantified, but it poses an interesting research idea for further analysis. 
The humidity diagram is heavily skewed towards high humidities. 

\subsection*{Reflection}
Although the topics presented this week were mostly nothing new to me, I found applying explorative data analysis quite interesting. 
It was quite fascinating to see how much knowledge can already be gained from a complex dataset by just looking at statistical properties of the data and its correlation. 
This is especially true, as I do not have a background in environmental science and therefore discovering correlations between different gases in the atmosphere or temperature and humidity did always come as a surprise and led to the curiosity to read up on the phenomena happening in the atmosphere that could lead to these links. 
My learning itself was quite well-structured this week. 
I was successful in writing the learning diary in multiple sessions and doing small updates at a time which also lead to me thinking more about the presented topics and dataset and also re-discovering topics that I already learned in previous courses which fit nicely into this week's lecture. 
Having a stronger background in statistics I do however think that the meaning of the sample variance and mean as unbiased \emph{estimators} of the true and unknown population quantities could have been motivated more thoroughly and clearly. 
I feel like this is one of the most important concepts to understand when starting to discuss statistics, as for people having no background so far these definitions must have come out of nowhere. 
I do however also understand the time constraints of this course, as it is a synthesis of two different courses which were held prior. 
Additionally, I think it would have been interesting to be required to do some analysis/coding myself for the exercises. 
The way the exercises are structured now, the entire code is already provided and only the description part is required. 
I feel like more interesting and maybe more rewarding insights could be gained if students are asked to perform some exploration on their own. 
This will maybe be the case in future exercises. 