\subsection*{Lecture}
In this weeks lecture we discussed validation for regression models or classifiers, performance indicators (both visually and numerically), linear regression and the basics behind neural networks (NNs), especially mulitlayer perceptrons (MLPs). 
As all topics have been a large part in prior courses I took, I will focus on giving a rough overview and highlighting approaches that are new to me and especially interesting. \\

We started by contrasting different validation approaches which is very important, as evaluating any model on data that has been used in \emph{constructing} the model usually leads to a much too optimistic performance. 
This is true both for machine learning task but also when doing simple regression. 
Splitting validation data from the training set can be done in different ways. 
For example one can perform hold-out validation, which means simply splitting the dataset into training and testing sets. 
The training set is used for constructing the model, while the testing set is used purely for evaluation. 
This approach is simple and fast, but the model performance is usually underestimated as the data used was not used for training. 
Another approach is k-fold cross-validation, where data is split into $k$ clusters. 
Training is done on $k-1$ clusters, the remaining one is used for validation. 
Then the validation cluster is cycled through the whole dataset, such that in the end every cluster has been used for training and validation. 
The overall performance is then the mean of all cluster performances. 
As a special case of k-fold cross validation one can also divide the data in as many clusters, as there are data points. 
This is then called leave-one-out cross-validation. 
It is numerically more expensive and usually provides an estimate that is too optimistic, but may be the best approach when the data sets are small. \\

We then went on to talk about performance indicators to evaluate the models we construct. 
A very simple approach is to use graphical methods that compare the model output with the observations themselves. 
This can be done using scatter plots or simple line graphs where the model and data are plotted on top of each other. 
I additionally used an approach extensively for lab courses I did in the past which was not covered in the material. 
When using regression, one can look at fit residuals, meaning the difference between the fit and data value. 
If a fit describes the data well, these residuals should be centered around zero and have a spread described by a chi-squared distribution. 
If one instead wants to evaluate the performance of a model numerically, one could look at the mean error, mean squared error or root mean squared error. 
These are shown in \autoref{eq:me, mae, rmse} from left to right. 
\begin{align}
    \mathrm{ME} &= \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i) & \mathrm{MAE} &= \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i| & \mathrm{RMSE} &= \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
    \label{eq:me, mae, rmse}
\end{align}
Here $y_i$ is a data point, $\hat{y}_i$ the model prediction and $n$ the total amount of data points. 
They all are of the same unit as the data itself and are therefore well interpretable. 
One can use the mean error to find the averaged signal deviation together with its direction, meaning one can find whether the model over- or underestimates the data. 
The MAE omits the directionality. 
Most often the RMSE is used however, as the squaring of the deviation has the nice property that large deviations contribute more to the error. 
One can also decompose the RMSE into statistical and systematic components. 
Other approaches we discussed where the coefficient of determination or $R^2$ value, and the index of agreement (IOA). 
The index of agreement is widely used, as it is unitless and lies between 0 and 1. 
It therefore allows for and easy comparison between different models. 
An index of agreement of 1 indicates a perfect model and one below $0.4$ indicates worse performance than random guessing. 
It is best practice to look at the IOA and RMSE together to find out if a model performs suitably well. \\

Next we discussed linear regression as a means of fitting curves with some free parameters to data. 
The summary here is again more qualitative, as fitting algorithms are implemented in many different Python packages and can easily be used. 
The idea behind fitting is to minimize the sum of all squared residuals, i.e. find parameters $a,b,c,\dots$ such that the following expression becomes minimal:
\begin{equation}
    \sum_{i=1}^N \left(\hat{y}_i\left(a,b,c,\dots\right) - y_i\right) ^2
\end{equation}
Here $\hat{y}$ denotes the model output again. 
How this minimization is done in practice is quite interesting in and of itself, but will not be explained further here. 
Uses of linear regression are to fit for example linear functions in order to extract simple trends from the data. 
But more complicated functions like the sum of different harmonics with prominent frequencies extracted from e.g. a periodogram can also be fitted to clean the data of yearly, weekly or hourly trends before further analysis. 
When doing this complicated fit with necessarily many free parameters, one should be careful to always use model validation to see if there is overfitting. 
Lastly there are also autoregressive models which employ a sort of "memory" to handle time series data better. 
An expansion of this are so called ARX models, which can handle the time series structure of different data columns simultaneously. 
I have never actually worked with such models, but they seem incredibly useful for the time series data that is usually present in environmental science. \\

During the last part of the lecture we had a quick introduction to neural networks with the example of a multilayer perceptron. 
This is a densely connected neural network that has one input layer and an output layer. 
Each node in the vector is assigned a weight vector and each layer provides an additional bias. 
The feed forward pass through the network updates each neuron in layer $j$ by performing a weighted sum of all neurons in the previous layer $j-1$ as such:
\begin{equation}
    x^{(j)} = \varphi\left(\sum_{\mathrm{n}} w_n^{(j-1)} x_n^{(j-1)}+b^{(j-1)}\right)
\end{equation}
Here $\varphi$ is a nonlinear activation function, the sum ranges over all neurons labelled $n$ in the previous layer, weights are denoted as $w$ and the bias as $b$. 
After showing the network a sample (or a batch of samples), one trains the network to perform better. 
Training works by so-called gradient descent during backpropagation. 
The idea is that a so-called loss function, a function that shows the deviation of the networks output from its desired output, is differentiated with respect to the model parameters. 
These are then updated towards the direction of steepest descent, meaning the model is adjusted such, that the output gets closer to the desired output. 
Further details are omitted here again. 
For training neural networks, validation is extremely important, especially because they can become quite prone to overfitting. 
To mitigate this, one should also experiment with the number of neurons of the network and their dimensionality and not set it too high. 
From my experience the best way to advance when it comes to machine learning is just to experiment, try different architectures, cost functions, activations and number of free parameters and see what works best. 

\subsection*{Exercises}
This week there were two exercises. 
The first dealt with creating a linear regression and ARX model and comparing them with each other as well as seeing how their performance change for different parameters. 
The second exercise deals with deep learning. \\

For the model comparison a finished notebook working with the \lstinline|AQData.csv| dataset was provided. 
Only parameters should be adjusted and the models performances should be compared. 
First, data is loaded and a standardized, sorted date time column is created. 
The test and validation datasets are then split. 
One year is always kept for validation, this is varied as a first test to see the influence of the choice of validation data. 
Four different years are used and compared: 1996, 1997, 1998 and 1999. 
Then a linear regression model with cosine and sine terms is constructed to model the NO$_2$ concentration. 
The amount of harmonics is kept fixed for the first part, I used 24 yearly, 24 weekly and 12 daily terms. 
This model is fit to the data and the changing validation set and its performance is evaluated. 
The results are shown in \autoref{tab:lr_model}. 
\begin{table}[ht]
\centering
\begin{tabular}{l|ccc| ccc}
    \toprule
    Year & IOA (Train) & $P_\mathrm{sys}$ (Train) & RMSE (Train) & IOA (Test) & $P_\mathrm{sys}$ (Test) & RMSE (Test) \\
    \midrule
    1999 & 0.74 & 0.60 & 18.54 & 0.71 & 0.56 & 18.76 \\
    1998 & 0.73 & 0.61 & 18.48 & 0.74 & 0.58 & 17.98 \\
    1997 & 0.76 & 0.57 & 18.06 & 0.68 & 0.53 & 19.72 \\
    1996 & 0.75 & 0.60 & 16.97 & 0.66 & 0.74 & 22.63 \\
    \midrule
    Mean & 0.75 & 0.60 & 18.01 & 0.70 & 0.60 & 19.77 \\
    \bottomrule
\end{tabular}
\caption{Index of agreement (IOA), proportion of systematic error ($P_\mathrm{sys}$) and RMSE for 1996--1999 and mean.}
\label{tab:lr_model}
\end{table}
As expected, the model performance varies slightly between different test years due to random chance. 
However, this fluctuation is quite small, indicating that overall the years are rather similar, and the model was able to generalize well. \\
Next, the test year of 1999 is kept fixed, but the number of harmonics is varied. 
I performed two test, one where I halved the number of terms (12 yearly and weekly terms, 6 daily ones) and one where I used 50\% more terms (36 yearly and weekly terms, 18 daily ones). 
Again performance tests are done and compared (see \autoref{tab:lr_model_nterms}). 
\begin{table}[ht]
\centering
\begin{tabular}{l|ccc|ccc}
    \toprule
    \# Terms & IOA & $P_\mathrm{sys}$ & RMSE & IOA & $P_\mathrm{sys}$ & RMSE \\
    & (Train) & (Train) & (Train) & (Test) & (Test) & (Test) \\
    \midrule
    12, 12, 6   & 0.64 & 0.64 & 19.43 & 0.64 & 0.76 & 22.62 \\
    24, 24, 12  & 0.74 & 0.60 & 18.54 & 0.71 & 0.56 & 18.76 \\
    36, 36, 18  & 0.77 & 0.57 & 16.47 & 0.66 & 0.72 & 22.77 \\
    \bottomrule
\end{tabular}
\caption{IOA, $P_\mathrm{sys}$ and RMSE for different numbers of terms.}
\label{tab:lr_model_nterms}
\end{table}
The test performed above illustrated under- and overfitting clearly. 
Using too few terms the index of agreement is small, and the systematic error proportion is large, especially for the test data. 
This is due to the model having too few parameters to suitably generalize and perform well. 
On the other hand, having too many parameters (bottom row) is also a problem as the IOA is again getting smaller with increasing systematic error. 
This is due to the model overfitting, i.e. losing the ability to generalize to unseen data because it could fit "too well" to the data it was trained on. \\

Next, an AR- and ARX model are built. 
As before, predictions should be made on the NO$_2$ concentration. 
The time lag was varied to see which one is best. 
For this different prominent peaks that were identified in \autoref{sec:data visualization} are used as time lags, namely 8, 12, 24 and 168 hours. 
Additionally, an ARX model including also the ozone concentration is trained and tested using the same time lags. 
The results are shown in 
\begin{table}[ht]
\centering
\begin{tabular}{l|cc|cc|cc|cc}
    \toprule
    & \multicolumn{2}{c|}{Train NO$_2$} & \multicolumn{2}{c|}{Test NO$_2$} & \multicolumn{2}{c|}{Train NO$_2$+O$_3$} & \multicolumn{2}{c}{Test NO$_2$+O$_3$} \\
    Lag (hours) & IOA & $P_\mathrm{sys}$ & IOA & $P_\mathrm{sys}$ & IOA & $P_\mathrm{sys}$ & IOA & $P_\mathrm{sys}$ \\
    \midrule
    8   & 0.66 & 0.71 & 0.65 & 0.71 & 0.67 & 0.70 & 0.66 & 0.69 \\
    12  & 0.66 & 0.71 & 0.65 & 0.71 & 0.67 & 0.70 & 0.66 & 0.68 \\
    24  & 0.67 & 0.70 & 0.66 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 \\
    168 & 0.74 & 0.62 & 0.72 & 0.61 & 0.74 & 0.61 & 0.73 & 0.59 \\
    \bottomrule
\end{tabular}
\caption{Index of agreement (IOA) and proportion of systematic error ($P_\mathrm{sys}$) for AR(X) models with varying time lags.}
\label{tab:arx_model_lag}
\end{table}
It can be seen that increasing the time lag leads to better models, as both the IOA is higher and the systematic error proportion lower. 
Additionally, including the ozone data also makes the model slightly better still, although it has less of an influence than increasing the memory to capture all prominent peaks. 
If simple models are preferred one should therefore use an AR model with sufficiently long memory to capture all strong variability. 
If one wants to create an even better, although more complex model, using an ARX model can be a good option. 
It should also be noted that fitting sine and cosine regressors leads to similar performance (when one keeps overfitting in mind and tries to mitigate it). 
This can therefore also be an option. 
Still, the full ARX model performs best both in terms of IOA and systematics and would therefore be my pick as the model to use. 
Not that the full tabularized test results showing also different metrics are available in \cite{github:stewey0-repo-2025} under \lstinline|Code\topic_5\LR_ARX_results.md|. \\

The second exercise this week dealt with a deep learning task using the same dataset. 
First, a date time column is again created and outliers are removed. 
Then three different architectures are used. 
First, a basic multilayer perceptron is used to predict the future NO$_2$ concentration based on a 24-hour lag of NO$_2$ data. 
In a second step further information is supplied to the net, namely periodic features in addition to the lagged variable. 
Lastly also other available meteorological data is added. 
In each step, I experiment with the network structure by adding and removing neurons or layers to find the optimal architecture by comparing their metrics on a test dataset. 
In a last step, all networks are summarized and compared. \\
The first network (NN1) only used 4 hidden neurons in a single hidden layer and the ReLu activation function and was trained only on lagged data. 
It achieved an IOA of 0.65 in both training and testing with a fraction of systematics PSE of 0.74 for training and 0.75 for testing. 
It was surprising to me, that such a small network with such limited data can already produce these relatively high scores. \\
For the next neural network periodic features are supplied additionally (NN2). 
The resulting test metrics for 1 layer with 4 neurons, 2 layers with 64 neurons each, 3 layers with 128, 64, 32 neurons and a changed activation function from ReLu to tanh are shown in \autoref{tab:mlp_comparison}. 
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
    \hline
	\textbf{Configuration} & \textbf{Train IA} & \textbf{Train PSE} & \textbf{Test IA} & \textbf{Test PSE} \\
    \hline
    \multicolumn{5}{l}{\textit{NN 1: NO$_2$ Lag Only}} \\
    (4) ReLu & 0.65 & 0.73 & 0.65 & 0.74 \\
\hline
\multicolumn{5}{l}{\textit{NN 2: NO$_2$ Lag + Periodic Features}} \\
(4) ReLU & 0.68 & 0.70 & 0.70 & 0.66 \\
(64, 64) ReLU & 0.80 & 0.53 & 0.76 & 0.50 \\
(128, 64, 32) ReLU & 0.80 & 0.50 & 0.76 & 0.46 \\
(128, 64, 32) tanh & 0.84 & 0.47 & 0.75 & 0.45 \\
\hline
\multicolumn{5}{l}{\textit{NN 3: NO$_2$ Lag + Periodic + Meteorological}} \\
(8, 4) ReLu & 0.77 & 0.57 & 0.77 & 0.51 \\
(64, 64) ReLu & 0.89 & 0.37 & 0.80 & 0.30 \\
(128, 64, 32) ReLu & 0.93 & 0.21 & 0.76 & 0.23 \\
\hline
\end{tabular}
\caption{MLP Model Comparison: Index of Agreement (IA) and Proportion of Systematic Error (PSE)}
\label{tab:mlp_comparison}
\end{table}
As can be seen the best configuration is reached for the architecture $(128,64,32)$ using the ReLu activation function. 
This is because the test IOA is highest and the test PSE is lowest. 
Additionally, the training time using the ReLu function is significantly shorter (30 instead of 50 seconds) with similar results, therefore it should be preferred. \\
The last neural network (NN3) also had access to meteorological data. 
As can be seen in the comparison table, this significantly increases the IOA again. 
The best architecture is reached for a network with two layers consisting of 64 neurons each. 
It can be seen that through supplying more data to the network less layers are required to reach a network that generalizes well. 
Even the very small network of size $(8,4)$ already beats the largest network of architecture NN2. 
A comparison of the observed vs. predicted graphs resulting from the best 3 networks of the different supplied training data are shown for comparison in \autoref{fig:e5_nn}. 
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c c c}
    \subfloat{\includegraphics[width=0.3\textwidth]{plots/e5_nn1.pdf} }
    &
    \subfloat{\includegraphics[width=0.3\textwidth]{plots/e5_nn2.pdf} }
    &
    \subfloat{\includegraphics[width=0.3\textwidth]{plots/e5_nn3.pdf} }
    \end{tabular}
    \caption{The observed vs predicted graphs of the three neural network architectures is shown. As the units are arbitrary, they are left out. A qualitative comparison to the ideal case of the 1:1 line shows that NN3 performs the best.}
    \label{fig:e5_nn}
\end{figure}
Lastly, a comparison between all models built during this exercise will be made. 
For this the best model parameters from each technique will be used. 
The results are shown in \autoref{tab:final comp}. 

\begin{table}[h]
\centering
\begin{tabular}{l|cc}
    \hline
	\textbf{Model} & \textbf{Test IOA} & \textbf{Test PSE} \\
    \hline
    Linear regression & 0.71 & 0.56 \\
    AR & 0.72 &0.61 \\
    ARX & 0.73 & 0.59 \\
    NN1 & 0.65 & 0.74 \\
    NN2 & 0.76 & 0.46 \\
    NN3 & 0.80 & 0.30
\end{tabular}
\caption{Here the final comparison between all models based on test metrics is shown.}
\label{tab:final comp}
\end{table}
It can be clearly seen that the best model both in terms of IOA, and systematics is the neural network using meteorological, NO$_2$ lag and periodic information. 
I would therefore recommend this model for forecasting purposes. 

\subsection*{Reflection}
This last lecture was again rather interesting to me. 
Similar to prior lectures there were a number of things I already knew, but each topic also offered something new for me. 
Examples of this are certain validation approaches, performance indicators and the fitting of many trigonometric functions of fixed periods to model an underlying complex periodic time series. 
This expanded my knowledge and made connections between prior statistics and machine learning courses I heard. 
The exercise was quite nice as well, I think. 
Although one could not really focus too much on the implementation of the different models, it was good to see a model comparison and to have seen how to construct them in principle for future reference. 
To be able to compare all models via the IOA was especially fascinating, as I could then also see how the approaches I did not know beforehand compare to the others. \\

Learning itself was similarly to last week somewhat rushed, which is due to my deadline being earlier as I am an exchange student. 
However, I think I was able to work quite efficiently and find a good workflow to do the exercises especially, which consisted of using AI to format Jupyter-cell outputs into Markdown tables, and then summarizing these into LaTeX tables. 
It was way quicker to just check whether the tables were produced correctly (which worked surprisingly well out of the box) than to hand-make all tables, especially in LaTeX. 
By now I have additionally grown quite used to writing lecture summaries, so this also contributed to my efficiency. 
All in all, I am quite happy with the progress of this week. 


