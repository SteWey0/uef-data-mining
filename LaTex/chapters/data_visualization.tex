\subsection*{Lecture}
This week's lecture focussed on visualizing high dimensional data and working with data in time series. 
Especially the first part contained many new things to me, which is why I will put a bigger emphasis on it in the following paragraphs. 
If data is low dimensional, visualization via histograms, box plots, scatter plots, or other methods is straightforward and has been discussed in the chapters above already. 
A more interesting question is how very high dimensional data with maybe hundreds of entries (columns in a dataframe) can be visualized in a 2-dimensional space to get a grasp on clusters and for example prepare the data for clustering algorithms to obtain data classes. 
We discussed to examples of dimensionality reduction here, namely Sammon mapping and self organizing maps (SOMs). 
These are quite interesting to me, as I only knew about Principal component analysis (which has the disadvantage of being linear) t-SNE prior to this lecture. 
Both Sammon mapping and SOMs are nonlinear and can therefore capture the usually complex high dimensional manifolds spanned by the data but are still easy to understand. \\

Sammon mapping is based on the idea of pairwise conserving distances in the mapping of input vectors $\mathbf{x}\in \mathbb{R}^D$ to output vectors $\mathbf{y}\in \mathbb{R}^d$ with $d<D$ (usually $d=2$). 
Specifically, the algorithm is constructed such, that small distances are weighted more strongly. 
This means, it tries to achieve a mapping that preserves local neighborhoods well and therefore preserves grouping of the data points. 
Sammon mapping follows these steps:
\begin{enumerate}
    \item First, pairwise distances between all inputs $d_{ij}=\|\mathbf{x}_i-\mathbf{x}_j\|$ and all outputs $d'_{ij}=\|\mathbf{y}_i-\mathbf{y}_j\|$ are calculated. For the first iteration outputs can be initialized randomly. 
    \item A cost function is calculated as $E=\frac{1}{\sum_{i<j}d_{ij}}\sum_{i<j}\frac{(d_{ij}-d'_{ij})^2}{d_{ij}}$. Note how the denominator leads to a larger weight for small distances. The prefactor is just for normalization.
    \item This cost function is minimized through usual gradient descent and the output mappings are updated as $y_i\leftarrow y_i-\eta\frac{\partial E}{\partial y_i}$, where $\eta$ is a preset learning rate. 
\end{enumerate}
The downsides of Sammon mapping include its sensitivity to local minima leading to failed mappings (due to random initialization), and that the algorithm is rather slow ($\mathcal{O}(N^2)$ distance calculations). 
However, it is useful if distances should be preserved well. \\

Self organizing maps are an unsupervised neural network, where neurons are placed on a usually fixed, 2-dimensional grid. 
Each neuron has a weight vector $\mathbf{w}\in \mathbb{R}^D$ of the same dimensionality as the data $\mathbf{x}\in \mathbb{R}^D$. 
The main idea is to find for each input sample (row of the dataset) the closest neuron, i.e. the one where the weight matches closely the input value itself. 
This neuron and its surrounding are then updated to be more similar to the input. 
After learning one tries to achieve the situation where each neuron reflects one "typical" input sample with neighboring neurons in the 2-dimensional grid being similar. 
This way a high dimensional input space is represented in a flattened, although topologically similar space. 
The following steps are important:
\begin{enumerate}
    \item First, the best-matching unit (BMU), the closest neuron, is calculated: $\text{BMU}(\mathbf{x})=\arg\min_i\|\mathbf{x}-\mathbf{w}_i\|$
    \item The BMU and its neighbors are updated to be more similar to the input. The update of neuron $i$ is based on the distance of the neuron to the BMU $c$: 
    $\mathbf{w}_i(t+1)=\mathbf{w}_i(t)+\alpha(t)\,h_{ci}(t)\,(\mathbf{x}-\mathbf{w}_i(t))$ with the neighborhood function (Gaussian)
    $h_{ci}(t)=\exp\!\left(-\frac{\|\mathbf{r}_c-\mathbf{r}_i\|^2}{2\sigma(t)^2}\right)$. Here $\alpha(t)$ is the learning rate and $\sigma(t)$ the kernel size, specifying the weighting of distances. 
\end{enumerate}
After these steps have been repeated for many input samples and the network is trained, similar inputs appear as clusters in the low dimensional representation, while boundaries or gaps in the representation show dissimilar inputs. 
This can be especially well seen when plotting a heatmap of the so-called U-matrix for neuron $i$, defined as 
\begin{equation}
    \mathbf{U}(i)=\frac{1}{|N(i)|}\sum_{j\in N(i)}\|\mathbf{w}_i-\mathbf{w}_j\| 
    \label{eq:U matrix}
\end{equation}
as it contains the average distances between the neuron and its neighbors (The sum ranges over all neighboring neurons). 
If the entries are small, the values in the neighborhood are quite similar which signals a cluster, while large entries signify the boundary between clusters. \\

The last topic was dealing with time series data as it is quite abundant in environmental data. 
Most of the topics discussed were nothing new to me as I have learned about them and used them extensively for example during my Bachelors thesis, which is why I will mainly glance over them. 
First we discussed examples of visualizing time series data to find important structures. 
We mentioned: 
\begin{itemize}
    \item Autocorrelation of the signal. It describes the correlation between the signal and itself for different time lags and can therefore be used to get an intuition about cyclic structures. See \lstinline|numpy.correlate()| for further information.
    \item Plotting trajectories onto mappings obtained by SOM or Sammon mapping. This seems quite interesting to me, as one should be able to observe, how the data moves between clusters given a certain time lag. 
    \item Fast-Fourier-transforms (FFTs) to transform the time domain signal into frequency domain. See \lstinline|numpy.fft.rfft()| for real signals. 
    \item The complex result of an FFT can be visualized as a Periodogram, a real estimate of the power spectral density of the signal by either taking the modulus squared of the FFT result and applying proper normalization or using \lstinline|scipy.signal.periodogram()| directly. This is an important visualization tool, as one can easily identify the most dominant frequencies (like weekly or daily cycles) in the signal as peaks.
\end{itemize}
Lastly, we discussed filtering the signal to remove these cycles or slow trends for the further analysis. 
This can be done by:
\begin{itemize}
    \item A simple moving average. In effect this is a convolution of the signal with a boxcar function or a very primitive lowpass. It is rather primitive as high frequencies are not filtered well but very easy to compute in a first step. Better low-, high- and band pass filters are available e.g. in \lstinline|scipy.signal.butter()|, but they have limited usability in environmental data. 
    \item Seasonal differencing, i.e. subtracting the value one period (day, week, \dots) prior is often used, as both slow trends and the filtered frequency are eliminated. 
    \item Slow trends can often be extracting by subtracting a fitted line to the data. 
\end{itemize}
After filtering one should visualize the data again (e.g. with a Periodogram) to see how the filter affected the data. 


\subsection*{Exercise}
This week's exercise was about comparing different visualization techniques and finding out, what information they provide about a sample dataset. 
Here, an air quality dataset was provided. 
The data is imported as a \lstinline|pandas| dataframe and an additional standardized time column is created. 
Then four main topics are covered: First the distributions and outliers are analyzed using histograms and box plots, then the relations between variables are examined with scatter plots. 
After this, temporal patters are investigated using smoothing, autocorrelations and periodograms and lastly dimensionality reduction is used. \\

Looking only at the histograms one can see, that most distributions look more or less Gaussian when on keeps in mind that they are sometimes cut due to unphysical values. 
However, especially the PM10, as well as the humidity values seem to follow a different distribution. 
Additionally, one can see that many outliers are present in the NO$_2$, PM10 and CO histograms. 
These are visible as long tails with only a few entries in each bin. 
It seems also, that NO$_2$ is more variable than PM10, which is more peaked but also has larger outliers. 
Through changing the bin number, I was able to find that undersampling the data by using too few bins is not good, as much detail is lost, and it is hard to see a trend and outliers this way. 
On the other hand increasing the bin number too much is also not good as fine variability in the data starts to appear, which makes interpretation harder. 
Having some amount of averaging by collecting the data in e.g. 30 bins seems optimal. \\
Next, I took a look at the box plots of the same data shown in \autoref{fig:plots/e3_box.pdf}. 
\begin{figure}[htb]
    \centering
    \includegraphics{plots/e3_box.pdf}
    \caption{The boxplots of the air quality dataset are shown.}
    \label{fig:plots/e3_box.pdf}
\end{figure}
These unveiled that O$_3$ has the largest interquartile range (IQR), which could also be seen in the very broad histogram. 
This could be due to the amount of ozone in the atmosphere changing a lot (e.g. during the seasons) instead of staying at a more or less fixed level. 
The highest median is present in the humidity data, which was expected from the histogram already as well, as it seems to be nearly exponentially peaked at 100\%. 
The box plots excel at showing the spread and amount of outliers. 
It is immediately clear that PM10 and NO$_2$ have the largest amount of outliers in the dataset -- This was way harder to see when I looked at the histograms. 
Comparing the both one can see that while NO$_2$ has a larger IQR, it has less extreme outliers than PM10. \\

Using scatter plots like the one shown in \autoref{fig:plots/e3_scatter.pdf} correlations between variables were examined next. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e3_scatter.pdf}
    \caption{One scatter plot indicating correlation between variables is shown as an example.}
    \label{fig:plots/e3_scatter.pdf}
\end{figure}
When examining NO$_2$ and humidity, I could see a slight negative correlation, as the slope of a fitted line is negative. 
As there are many outliers in the NO$_2$ data which seem to influence the fit substantially, but the bulk of the data seems to be quite compact in a cloud, this simple fit does not yet provide compelling evidence in my opinion yet. 
One should maybe try to remove and quantify outliers first, before repeating the analysis. 
No clear trend can be seen when looking at the correlation between wind speed and the temperature columns. 
On the other hand, a strong positive correlation can be seen in \autoref{fig:plots/e3_scatter.pdf}, where PM10 seems to increase quite linearly with rising NO$_2$ levels over the whole measurement range. 
Using an additional color scale one can also see a strong anticorrelation between the wind speed and the NO$_2$ measurement. 
While PM10 and wind speed also seem slightly anticorrelated, this is less evident here, which can also be seen when looking at the two columns separately. \\

Next, I looked at temporal patters in the data. 
For this we started with cleaning the date time column and sorting it ascendingly. 
Then I looked at a simple rolling average filter taken over 24 hours to smooth daily fluctuations. 
This leads to more smooth, noise resistant curves where trends (or the lack thereof) can be seen more clearly. 
After smoothing the data seasonal trends can be easily seen in the temperature (peaks in summer), O$_3$ (peaks in spring), PM10 (spikes in the winter) and humidity (dips in summer). 
I could also spot a gradual increase in temperature and decrease in NO$_2$ and PM10 levels over the years by using averaging and a linear fit. 
This simple example shows, that although a rolling average filter is quite primitive, it can help extract interesting insights to the data quickly and can therefore be used as a straightforward first step. \\
The next task was to look at the autocorrelation of the signal. 
For this a function \lstinline|autocorr_series()| is called in the provided code but is not defined there. 
I implemented it myself as follows: 
\begin{lstlisting}[language=Python, caption={Implementation of \lstinline|autocorr_series| function}, label={lst:autocorr}]
def autocorr_series(y_ac, max_lag):
    autocors = np.zeros(max_lag + 1)
    lags = np.arange(max_lag + 1)
    for lag in lags:
        autocors[lag] = y_ac.autocorr(lag=lag)
    return lags, autocors
\end{lstlisting}
The autocorrelation shows peaks every 24 hours, indicating a daily cycle in the PM10 values. 
It should be noted that all peaks at multiples of 24 hours are just higher harmonics that stem from the fact that any 24 hour regularity in the signal also means there are 48,72,\dots hour regularities. 
In other words: If a value is similar the next day, it is again similar in two days. 
This shows there is a daily pattern in the emission of small particulates. 
This could for example be due to traffic, which also has daily repeating patters (many cars during rush hour, few in the night). \\
More information is revealed when looking at the periodogram of the PM10 data, which is shown in \autoref{fig:plots/e3_periodogram.pdf}. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e3_periodogram.pdf}
    \caption{The periodogram of the PM10 data is shown.}
    \label{fig:plots/e3_periodogram.pdf}
\end{figure}
Clear peaks can be seen at 8, 12, 24 and 168 hours period. 
This indicates that additional to the daily cycle, there are also cycles that are a third, half and 7 days long. 
Following the traffic-hypothesis as an explanation to the PM10 cycles, the weekly cycle could be due to Sundays having generally less traffic as most people do not need to work. 
The small 8 and 12-hour cycles could maybe be explained by a peak in traffic during the morning and evening, before and after the 8-12 hour work day is completed. 
These are however of course only hypotheses which need further testing and more data to be tested. 
As an example, testing a possible correlation between traffic and this air quality data could be rather interesting. 
Ozone and nitrogen dioxide data show similar cycles as the PM10 data, although ozone has a much more pronounced 12-hour cycle. 
This could be due to how ozone is created in the atmosphere. \\

As a last exercise, a 2-dimensional Sammon plot has been created. 
As the underlying algorithm tries to keep distances (especially small ones) similar when encoding, one expects that close data points remain close even after the mapping to 2D. 
I have tried many different settings and plotting color scales for all different columns of the dataframe, however after encoding no clear clusters emerged from the data. 
Instead, the map produces quite spread out, seemingly random points, indicating low clustering in the data. 
Through changing the z-score value, I was able to verify that a smaller z-score can be used to remove more outliers. 

\subsection*{Reflection}
I found this weeks material to be quite interesting again. 
Especially the topics about dimensionality reduction were quite new to me. 
Handling time series data has been something I have done a lot during my prior studies, so I was quite familiar with Fourier transforms and correlations between signals already, still it was nice seeing it applied again in a slightly different manner. 
However, I found the exercises this week to be a bit too long. 
Instead of having this many tasks I feel like it would have been more interesting to focus on a few applications specifically and go more into depth about them. 
Besides the one missing function implementation (see \autoref{lst:autocorr}), nothing needed to be programmed. 
For me, the concepts of this week would have been better internalized, if I had been confronted with the (maybe already cleaned and sorted) dataset, and were asked to perform simple steps like a Fourier transform myself, i.e. if the provided notebook would serve more as a guide and less as finished implementation of the data mining process. 
However, I see that this may be difficult to realize due to different programming backgrounds of the students. 
I also think, that fewer questions would have sufficed, as it is rather difficult to answer them all on only 2 pages or so. 
For me this is especially true as the questions often dealt with the specific air quality dataset at hand which while interesting, should not be the main focus of the exercise in my opinion. 
Instead, it would have been more illustrative to learn about the Data Mining tools themselves more, by programming a bit on my own or needing to compare different approaches, as these tools can be applied to many forms of data. \\
This week I managed to structure my learning better again. 
I did a little each day, which definitely helped my understanding and in answering the questions for the exercises. 
However, I underestimated the amount of time needed for the exercises by a lot this week. 
As they were a fair bit longer this week, it got quite tight to the end and I needed to work quite a lot two days. 
Next week I should try to plan more time for the exercise. 
However, I am quite happy with my progress in general, as I up to now succeeded in working through the lectures, exercises and writing the diary each week. 