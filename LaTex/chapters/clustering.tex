\subsection*{Lecture}
This week's lecture dealt with clustering data. 
Clusters represent continuous regions in a $p$-dimensional space with high data density, separated by regions of lower density. 
They are characterized by their density, variance, shape, and separation. 
Clustering is needed to discover patterns in the data, compress it, or detect anomalies. 
We discussed 3 different algorithms this week and the need for validation. 
These algorithms belong to the following classes:
\begin{itemize}
        \item \textbf{Sequential algorithms}: These are usually quite fast and process points in their order of appearance. We discussed nearest neighbor (NN) clustering. 
        \item \textbf{Hierarchical algorithms}: These build a dendrogram (a tree of nested clusters by either agglomerating or dividing the data into groups. 
        \item \textbf{Optimization algorithms}: These define a cost function that is iteratively optimized. We discussed k-means. These algorithms are usually well scalable and suited for large datasets. 
\end{itemize}
All algorithms work differently and have their up- and downsides, which are discussed in the following. 
I focus here on giving a more qualitative view, as most algorithms are already implemented in Python libraries, and it makes understanding much simpler. \\

The core idea of NN-clustering is that points that are close in space should belong to the same cluster.
The procedure is as follows:
\begin{enumerate}
        \item Present vectors one at a time.
        \item For each new vector $\mathbf{x}_i$, find nearest already clustered point $\mathbf{x}_m$.
        \item If $d(\mathbf{x}_i,\mathbf{x}_m) \leq \tau$ (threshold), assign to that cluster. Otherwise, create a new cluster.
 \end{enumerate}
One can use different metrics to quantify distance, but usually the Euclidean distance is used. 
The strength of this algorithm is that it is easy, fast and one does not need a predefined number of clusters. 
However, the result is strongly dependent on input order, highly sensitive to the chosen threshold and tends to lose the global view of the data as only local similarity is used in the metric. 
It is also prone for chaining together nearby data points and bridge different clusters. \\

The main idea behind hierarchical clustering is to build a dendrogram based on pairwise distances. 
Our discussion focused on the agglomerative approach where starting with singletons more and more clusters are merged based on distances. 
The algorithm is as follows:
\begin{enumerate}
    \item Compute initial distance matrix $D$.
    \item Repeatedly merge the two clusters with minimum inter-cluster distance.
    \item Update distance matrix using chosen linkage criterion.
\end{enumerate}
There are different linkage criteria such as single, complete and average linking. 
The strength of hierarchical clustering is that it produces a full hierarchy instead of single partitions. 
However, it is computationally quite expensive, it cannot undo early, unideal merges, and is sensitive to the chosen distance metric and linkage choice. 
To me, it seems, that it may be a sensible algorithm as a first choice for small datasets as the full structure may offer additional insights that can be explored further. \\

The last algorithm we discussed is the k-means algorithm, an optimization based approach, meaning that the data is partitioned into $k$ groups by iterative optimization. 
It works roughly as follows:
\begin{enumerate}
    \item Initialize $k$ centers (randomly).
    \item Assign each point to the nearest center.
    \item Update centers as means of assigned points.
    \item Repeat until assignments no longer change.
\end{enumerate}
It is comparably fast and scalable to large datasets and produces compact clusters when data is roughly spherical. 
The algorithm is however not well suited for data that is not spherical, struggles with overlapping groups and outliers and is sensitive to initialization. 
Additionally specifying the number of clusters beforehand is required. 
This can be however optimized during the clustering validation, which we discussed next. \\

Validation of the clustering result is extremely important as the procedures are unsupervised and usually require user inputs like the amount of centers $k$, linkage criteria or distance metrics. 
It is therefore important to find out whether clustering worked well or not. 
Validation can be done externally (if known labels are present), internally (measure compactness statistically) and relatively (compare clustering obtained by the same algorithm for different parameters). 
We focused on the relative scheme, which to me also seems to be the most applicable as it can be used for all data regardless of labels. 
For algorithms requiring some input number $n_c$ (for example $n_c=k$ for k-means) validation looks like this:
\begin{enumerate}
    \item Choose a range of numbers $n_c \in [n_{\min}, n_{\max}]$.
    \item For each $n_c$, run the clustering algorithm $r$ times  
          (different random initializations).
    \item Compute a validity index for each run.
    \item For each $n_c$, keep the best index value.
    \item Plot the index value against $n_c$ and select the best.
\end{enumerate}
The best number is usually the one where the index either changes sharply or has a minima or maxima. 
I find it very interesting, that using validation one can also find out, whether the data possesses cluster structure at all. 
If the index does not seem to change at all, this would suggest data that is not clustered. 
We discussed two different validity indexes. 
The Davies-Bouldin-Index (DB-index) shows the average similarity between clusters. 
Naturally, for the clustering to have worked well, one wants a minimum in the index. 
It is especially good for comparing clusters with different $k$ during k-means clustering as it captures spherical, separated clusters best. 
Additionally, one can also use the Silhouette measure which assesses how well each data points fits into its assigned cluster. 
It is ranged between -1 and 1 where a large positive value indicates that the point is much closer to its own cluster than to any other (good clustering), while a value of -1 means the opposite and that the point is probably misclassified. 
To compare clustering, one can use a Silhouette plot which visually shows outliers as negative values and whether some clusters might be overlapping or poorly defined. 
In practice, it is best to use both indices and look for an agreement between them. 

\subsection*{Exercise}
This week's exercise was a small hands-on example of the workflow behind clustering data based on the \lstinline{toolo.csv} dataset. 
After loading the data that was already used in exercise 1, we explored different ways of normalizing the entries first. 
For this we looked at variance, minmax, and Euclidean scaling (meaning dividing each row by its L2 norm). 
To see which pre-transform worked best, we then used different metrics. 
For a start the DB and Silhouette indices are calculated and tabulated. 
After this k-mean clustering with 7 clusters is employed to compare Sammon maps, box plots and Silhouette plots graphically. 
All three methods agree that Euclidean scaling is best suited for the dataset at hand. 
Firstly, the calculated Silhouette index is largest for Euclidean scaling and the calculated DB-index is smallest. 
Although the difference to the other normalization indices is quite small (roughly in the range of 0.1 for both) it is already a good indication, that Euclidean normalization might be best suited. 
Additionally, the obtained Sammon mappings show clearly, that the clusters are very tight together for minmax and variance scaling, while they cover a much larger parameter space in the mapping for Euclidean normalization. 
This can be seen in \autoref{fig:plots/e4_sammon.pdf}. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e4_sammon.pdf}
    \caption{The obtained Sammon mapping of 7 clusters is shown for Euclidean normalization. Note that clusters are well separated and A large parameter space is used. This is not the case for other scalings.}
    \label{fig:plots/e4_sammon.pdf}
\end{figure}
The distributions of the different variables seem to show a similar variability as well when using L2-normalization. 
Lastly, when looking at the Silhouette plots, one can see, that for Euclidean normalization nearly all scores are positive and clusters are well separated. 
For the following tasks, I will therefore use Euclidean normalization. 
The Silhouette plot also suggests that there may be some overlap between clusters 5 and 6, as they are less clearly separated than the other clusters, which can be seen in the attached plot (\autoref{fig:plots/e4_silhouette.pdf}). 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e4_silhouette.pdf}
    \caption{The Silhouette plot when using seven clusters and Euclidean normalization is shown.}
    \label{fig:plots/e4_silhouette.pdf}
\end{figure}
The Sammon embedding suggests mostly spherical/globular clusters, although there are also some outliers. 
All in all, I think k-mean clustering is well suited. \\

Having decided on preprocessing method and clustering technique, the next sensible step is to find the best number of clusters by comparing the resulting DB-indices. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e4_dbi.pdf}
    \caption{The resulting DBI for different number of clusters can be seen. $k=2$ performs best.}
    \label{fig:plots/e4_dbi.pdf}
\end{figure}
As can be seen in \autoref{fig:plots/e4_dbi.pdf}, 2 clusters seem to work best. 
This is also true for different random initializations, the algorithm seems quite stable. 
As the code has been designed to work with the lowest DB-index and start the search with two clusters, I will therefore do all following analysis with 2 clusters. 
I will reflect if this is sensible in the end. \\

In the last step we looked at the temporal evolution of the clusters. 
\begin{figure}[htbp]
    \centering
    \includegraphics{plots/e4_timeseries.pdf}
    \caption{The temporal evolution of O$_3$ clusters can be seen. Trivially, having two clusters works well, as high-emission periods are separated from low-emission ones.}
    \label{fig:plots/e4_timeseries.pdf}
\end{figure}
As can be seen in \autoref{fig:plots/e4_timeseries.pdf} exemplary, the 2 clusters can be easily understood. 
One corresponds to the periods where high emission of pollutants was present, while the other corresponds to low emission periods. 
This makes intuitive sense: If only two clusters are available periods where there are correlated high emissions (e.g. because of traffic) belong to one group, while the opposite belongs to the other. 
I think this traffic behavior is exactly what can be seen in the cluster time evolution. 
One could therefore give the clusters the labels "high emission" and "low emission". 
From the analysis of just two clusters not a lot of insights can be gained. 
One could inspect times of high pollution and try to find policies against that by correlating these times with processes that might be happening. 
Further analysis, like the correlation with traffic data, is needed. 


\subsection*{Reflection}
The lecture material this week was quite interesting, as it was something completely new to me. 
It felt like it fit well with the topics prior to and after that. 
The exercises were also a great addition to the lectures. 
I liked to be able to play around a bit with different normalizations and amount of clusters. 
However, I think that more interesting insights on the data could be gained when clustering into more than two groups is enforced. 
In the beginning I was quite surprised, that two clusters had such a low DN-score and performed best.
However, after looking at the time traces and visualizing some data it became clear to me, why that is the case. 
Although one could maybe say that 2 clusters did not really provide much insight, this did still help me understand clustering and plotting the time traces quite a lot by working through my initial confusion. 
Learning and writing the diary was a bit hectic for me this week. 
AS my studies as an exchange student soon reach an end, I tried to fit more than one week of lectures and exercises into this week, which made it rather challenging. 
Still, I am quite happy with my progress. 
